{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Data Exploration\n",
    "Notebook to test data analysis and processes before transferring to Python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import isnan, count, when, col, desc, udf, col, sort_array, asc, avg, regexp_replace, from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark Local Analysis Development\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(filepath):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root,'*.json'))\n",
    "        for f in files :\n",
    "            all_files.append(os.path.abspath(f))\n",
    "    \n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_files = get_files('data/song_data')\n",
    "log_files = get_files('data/log-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define JSON import schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_schema = StructType([StructField('num_songs', IntegerType(), False),\n",
    "                     StructField('artist_id', StringType(), False),\n",
    "                     StructField('artist_latitude', DoubleType(), False),\n",
    "                     StructField('artist_longitude', DoubleType(), False),\n",
    "                     StructField('artist_location', StringType(), False),\n",
    "                     StructField('artist_name', StringType(),False),\n",
    "                     StructField('song_id', StringType(), False),\n",
    "                     StructField('title', StringType(), False),\n",
    "                     StructField('duration', FloatType(), False),\n",
    "                     StructField('year', IntegerType(), False)])\n",
    "\n",
    "log_schema = StructType([StructField('artist', StringType(), True),\n",
    "                     StructField('auth', StringType(), True),\n",
    "                     StructField('firstName', StringType(), True),\n",
    "                     StructField('gender', StringType(), True),\n",
    "                     StructField('itemInSession', StringType(), True),\n",
    "                     StructField('lastName', StringType(),True),\n",
    "                     StructField('length', StringType(), True),\n",
    "                     StructField('level', StringType(), True),\n",
    "                     StructField('location', StringType(), True),\n",
    "                     StructField('method', StringType(), True),\n",
    "                     StructField('page', StringType(), True),\n",
    "                     StructField('registration', StringType(), True),\n",
    "                     StructField('sessionId', StringType(), True),\n",
    "                     StructField('song', StringType(), True),\n",
    "                     StructField('status', StringType(), True),\n",
    "                     StructField('ts', StringType(), True),\n",
    "                     StructField('userAgent', StringType(), True),\n",
    "                     StructField('userId', StringType(), True)])\n",
    "\n",
    "## There is something wrong in the data in the log jsons that throw an error when trying to cast as a non-String \n",
    "## type at import which causes the entire row to be nulled.  For this reason, we'll still keep consistant with \n",
    "## song_data and define a schema but for log_schema all types will be String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data = spark.read.schema(song_schema).option(\"multiline\",\"true\").json(song_files)\n",
    "log_data = spark.read.schema(log_schema).option(\"multiline\",\"true\").json(log_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "From previous inspection of this data, we know there are multiple null entries as well as invalid year entries (reads as 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data_clean = song_data.na.drop()\n",
    "log_data_clean = log_data.na.drop()\n",
    "song_data_clean = song_data_clean.filter(song_data_clean.year != \"0\")\n",
    "log_data_clean = log_data_clean.filter(log_data_clean.userId != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data_clean.createOrReplaceTempView(\"song_table\")\n",
    "log_data_clean.createOrReplaceTempView(\"log_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "          SELECT *\n",
    "          FROM song_table\n",
    "          '''\n",
    "         ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert epoch_seconds to timestamp\n",
    "log_data_clean.select(col(\"ts\").cast(IntegerType())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data_clean.select(col(\"ts\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripDQ(string):\n",
    "    return string.replace('\"', \"\")\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "udf_stripDQ = udf(stripDQ, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = log_data_clean.withColumn(\"ts\", udf_stripDQ(log_data_clean[\"ts\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test.select(test[\"ts\"].asType('int').alias(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test.withColumn('test', regexp_replace('ts',\"[^0-9]\",\"\").cast(IntegerType())).select(\"ts\",\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_timestamp = udf(lambda x: x/1000, TimestampType())\n",
    "test2 = test.withColumn('start_time',get_timestamp(test.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datetime column from original timestamp column\n",
    "get_datetime = udf(lambda x: from_unixtime(x/1000), TimestampType())\n",
    "test2 = test.withColumn('datetime', get_datetime('ts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test.withColumn(col('ts')).cast(IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
